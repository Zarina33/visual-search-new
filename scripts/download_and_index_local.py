#!/usr/bin/env python3
"""
–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ S3 –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è.
–ù–ï —É–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/download_and_index_local.py --limit 100  # —Ç–µ—Å—Ç
    python scripts/download_and_index_local.py              # –≤—Å–µ
"""
import asyncio
import sys
from pathlib import Path
from typing import List, Tuple, Optional
from tqdm import tqdm
import time
from PIL import Image
import io

sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from app.utils.bakai_s3_client import BakaiS3Client
from app.models.clip_model import CLIPEmbedder, CLIPModel
from app.db.qdrant import QdrantManager
from app.db import get_session, create_product
from app.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
LOCAL_STORAGE = Path.home() / "product-images"  # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
BASE_URL = "http://localhost/images"  # Base URL –¥–ª—è nginx
BUCKET_NAME = "product-images"
BATCH_SIZE = 32
MIN_IMAGE_SIZE = 50


def extract_product_id(s3_key: str) -> str:
    """–ò–∑–≤–ª–µ—á—å ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ –ø—É—Ç–∏."""
    parts = s3_key.split('/')
    if len(parts) >= 2:
        return parts[0]
    return "unknown"


def is_main_image(s3_key: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —ç—Ç–æ –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
    filename = Path(s3_key).stem
    return filename.endswith('_1')


def validate_and_save_image(
    image_data: bytes,
    product_id: str,
    filename: str
) -> Optional[Path]:
    """–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        if len(image_data) < 1000 or len(image_data) > 20 * 1024 * 1024:
            logger.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è {product_id}/{filename}: {len(image_data)} –±–∞–π—Ç")
            return None
        
        # –û—Ç–∫—Ä—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        try:
            img = Image.open(io.BytesIO(image_data))
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–æ
            img.verify()
            # –ü–æ—Å–ª–µ verify() –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ—Ç–∫—Ä—ã—Ç—å
            img = Image.open(io.BytesIO(image_data))
        except Exception as e:
            logger.warning(f"–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {product_id}/{filename}: {e}")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if img.size[0] < MIN_IMAGE_SIZE or img.size[1] < MIN_IMAGE_SIZE:
            logger.warning(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ {product_id}/{filename}: {img.size}")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
        if img.format not in ['JPEG', 'PNG', 'WEBP', 'BMP']:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç {product_id}/{filename}: {img.format}")
            return None
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ RGB
        if img.mode != 'RGB':
            try:
                rgb_img = Image.new('RGB', img.size, (255, 255, 255))
                if img.mode in ('RGBA', 'LA'):
                    rgb_img.paste(img, mask=img.split()[-1])
                else:
                    rgb_img.paste(img)
                img = rgb_img
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ RGB {product_id}/{filename}: {e}")
                return None
        
        # –°–æ–∑–¥–∞—Ç—å –ø—É—Ç—å
        product_dir = LOCAL_STORAGE / product_id
        product_dir.mkdir(parents=True, exist_ok=True)
        
        save_path = product_dir / f"{filename}.jpg"
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
        try:
            img.save(save_path, 'JPEG', quality=85, optimize=True)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è –∏ –º–æ–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å
            test_img = Image.open(save_path)
            test_img.verify()
            test_img.close()
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {product_id}/{filename}: {e}")
            if save_path.exists():
                save_path.unlink()  # –£–¥–∞–ª–∏—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            return None
        
        return save_path
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {product_id}/{filename}: {e}")
        return None


async def download_all_images(s3_client: BakaiS3Client, limit: Optional[int] = None):
    """
    –°–∫–∞—á–∞—Ç—å –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ S3 –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É.
    
    Returns:
        List[(product_id, local_path)]
    """
    print("\n" + "=" * 70)
    print("üì• –°–ö–ê–ß–ò–í–ê–ù–ò–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô –ò–ó S3")
    print("=" * 70)
    
    # –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    print("\nüìã –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤...")
    all_objects = []
    continuation_token = None
    
    while True:
        if continuation_token:
            response = s3_client.s3_client.list_objects_v2(
                Bucket=BUCKET_NAME,
                ContinuationToken=continuation_token,
                MaxKeys=1000
            )
        else:
            response = s3_client.s3_client.list_objects_v2(
                Bucket=BUCKET_NAME,
                MaxKeys=1000
            )
        
        objects = response.get('Contents', [])
        all_objects.extend(objects)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_objects)}...", end='\r')
        
        if not response.get('IsTruncated'):
            break
        
        continuation_token = response.get('NextContinuationToken')
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ S3: {len(all_objects)}")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    all_images = []
    for obj in all_objects:
        key = obj['Key']
        product_id = extract_product_id(key)
        all_images.append({
            'product_id': product_id,
            'key': key,
            'size': obj['Size']
        })
    
    print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(all_images)}")
    
    if limit:
        all_images = all_images[:limit]
        print(f"‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {limit} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –°–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print("\nüì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    LOCAL_STORAGE.mkdir(parents=True, exist_ok=True)
    
    downloaded = []
    skipped = 0
    failed_validation = 0
    failed_download = 0
    
    for img in tqdm(all_images, desc="Downloading", unit="img"):
        product_id = img['product_id']
        key = img['key']
        filename = Path(key).stem
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–∞–π–ª —É–∂–µ –Ω–µ —Å–∫–∞—á–∞–Ω
        expected_path = LOCAL_STORAGE / product_id / f"{filename}.jpg"
        if expected_path.exists():
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω
            try:
                test_img = Image.open(expected_path)
                test_img.verify()
                test_img.close()
                downloaded.append((product_id, expected_path))
                skipped += 1
                continue
            except Exception:
                # –§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, —É–¥–∞–ª–∏—Ç—å –∏ —Å–∫–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ
                logger.warning(f"–°—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø–µ—Ä–µc–∫–∞—á–∏–≤–∞–µ–º: {expected_path}")
                expected_path.unlink()
        
        try:
            # –°–∫–∞—á–∞—Ç—å –∏–∑ S3
            response = s3_client.s3_client.get_object(Bucket=BUCKET_NAME, Key=key)
            image_data = response['Body'].read()
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
            local_path = validate_and_save_image(image_data, product_id, filename)
            
            if local_path:
                downloaded.append((product_id, local_path))
            else:
                failed_validation += 1
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {key}: {e}")
            failed_download += 1
            continue
    
    print(f"\n‚úÖ –°–∫–∞—á–∞–Ω–æ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {len(downloaded)}")
    if skipped > 0:
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ –µ—Å—Ç—å): {skipped}")
    if failed_validation > 0:
        print(f"‚ö†Ô∏è  –ù–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: {failed_validation}")
    if failed_download > 0:
        print(f"‚ùå –û—à–∏–±–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {failed_download}")
    
    return downloaded


async def generate_embeddings(
    embedder: CLIPEmbedder,
    images: List[Tuple[str, Path]]
) -> List[Tuple[str, list]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
    print("\n" + "=" * 70)
    print("üß† –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
    print("=" * 70)
    
    embeddings = []
    
    for i in tqdm(range(0, len(images), BATCH_SIZE), desc="CLIP", unit="batch"):
        batch = images[i:i + BATCH_SIZE]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—É—Ç–∏ –¥–ª—è batch
        batch_paths = [str(img_path) for _, img_path in batch]
        batch_ids = [product_id for product_id, _ in batch]
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–µ–º
            batch_embeddings = await embedder.generate_embeddings_batch(batch_paths)
            
            # –î–æ–±–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for product_id, embedding in zip(batch_ids, batch_embeddings):
                if embedding is not None:
                    embeddings.append((product_id, embedding.tolist()))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ batch —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            # Fallback: –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ –æ–¥–Ω–æ–º—É
            for product_id, img_path in batch:
                try:
                    embedding = await embedder.generate_embedding(str(img_path))
                    if embedding is not None:
                        embeddings.append((product_id, embedding.tolist()))
                except Exception as e2:
                    logger.error(f"–û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ {product_id}: {e2}")
                    continue
    
    print(f"\n‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
    return embeddings


async def save_to_databases(
    images: List[Tuple[str, Path]],
    embeddings: List[Tuple[str, list]]
):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ PostgreSQL –∏ Qdrant."""
    print("\n" + "=" * 70)
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–ê–ó–´ –î–ê–ù–ù–´–•")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embeddings_dict = {pid: emb for pid, emb in embeddings}
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
    products_data = []
    qdrant_data = []
    seen_products = set()  # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    
    for product_id, img_path in images:
        if product_id not in embeddings_dict:
            continue
        
        # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
        if product_id in seen_products:
            continue
        seen_products.add(product_id)
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–π URL
        relative_path = img_path.relative_to(LOCAL_STORAGE)
        #local_url = f"{BASE_URL}/{relative_path}"
        local_url = f"/images/{relative_path}"
        # –î–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–∞
        product_data = {
            "external_id": f"bakai_{product_id}",
            "title": f"Product {product_id}",
            "description": f"BakaiMarket product ID: {product_id}",
            "category": "bakai",
            "price": None,
            "currency": None,
            "image_url": local_url,
            "product_metadata": {
                "source": "s3",
                "product_id": product_id,
                "local_path": str(img_path)
            }
        }
        
        products_data.append(product_data)
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è Qdrant
        qdrant_data.append({
            "id": f"bakai_{product_id}",
            "vector": embeddings_dict[product_id],
            "payload": {
                "external_id": f"bakai_{product_id}",
                "original_id": product_id
            }
        })
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ PostgreSQL
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ PostgreSQL...")
    saved = 0
    skipped = 0
    
    from sqlalchemy import select
    from app.db.postgres import Product, get_product_by_external_id
    
    for product_data in tqdm(products_data, desc="PostgreSQL", unit="—Ç–æ–≤–∞—Ä"):
        try:
            # –°–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
            async with get_session() as session:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ
                existing = await get_product_by_external_id(session, product_data['external_id'])
                if existing:
                    skipped += 1
                    continue
                
                await create_product(session, product_data)
                saved += 1
        except Exception as e:
            # Ignore duplicates
            if 'duplicate key' in str(e).lower():
                skipped += 1
    
    print(f"‚úÖ PostgreSQL: {saved}/{len(products_data)}")
    if skipped > 0:
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç—ã): {skipped}")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Qdrant
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant...")
    qdrant = QdrantManager()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –≤ __init__
    
    for i in tqdm(range(0, len(qdrant_data), 1000), desc="Qdrant", unit="batch"):
        batch = qdrant_data[i:i+1000]
        
        # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ø–∏—Å–∫–∏
        product_ids = [item['id'] for item in batch]
        vectors = [item['vector'] for item in batch]
        payloads = [item['payload'] for item in batch]
        
        await qdrant.upsert_vectors(product_ids, vectors, payloads)
    
    print(f"‚úÖ Qdrant: {len(qdrant_data)} –≤–µ–∫—Ç–æ—Ä–æ–≤")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(description="–°–∫–∞—á–∞—Ç—å –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    parser.add_argument("--limit", type=int, help="–õ–∏–º–∏—Ç —Ç–æ–≤–∞—Ä–æ–≤")
    args = parser.parse_args()
    
    print("\n" + "=" * 70)
    print("üöÄ –°–ö–ê–ß–ò–í–ê–ù–ò–ï –ò –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
    print("=" * 70)
    print(f"\nüìÅ –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {LOCAL_STORAGE}")
    print(f"üîó Base URL: {BASE_URL}")
    print(f"üì¶ S3 Bucket: {BUCKET_NAME}")
    
    start_time = time.time()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    print("\n‚öôÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
    s3_client = BakaiS3Client()
    embedder = CLIPEmbedder(device=settings.clip_device)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ")
    
    # 1. –°–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    images = await download_all_images(s3_client, limit=args.limit)
    
    if not images:
        print("\n‚ùå –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    # 2. –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings = await generate_embeddings(embedder, images)
    
    if not embeddings:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
        return
    
    # 3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
    await save_to_databases(images, embeddings)
    
    # –ò—Ç–æ–≥–∏
    elapsed = time.time() - start_time
    print("\n" + "=" * 70)
    print("üìä –ò–¢–û–ì–ò")
    print("=" * 70)
    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(images)}")
    print(f"üß† –°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed:.2f} —Å–µ–∫ ({elapsed/60:.2f} –º–∏–Ω)")
    if len(images) > 0:
        print(f"üìà –°–∫–æ—Ä–æ—Å—Ç—å: {len(images)/elapsed:.2f} —Ç–æ–≤–∞—Ä–æ–≤/—Å–µ–∫")
    
    print(f"\nüìÅ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {LOCAL_STORAGE}")
    print(f"üíæ –†–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏: ", end="")
    
    # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ä–∞–∑–º–µ—Ä –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
    all_files = list(LOCAL_STORAGE.rglob("*.jpg"))
    if all_files:
        total_size = sum(f.stat().st_size for f in all_files)
        if total_size > 1024**3:
            print(f"{total_size / 1024**3:.2f} GB")
        else:
            print(f"{total_size / 1024**2:.2f} MB")
        print(f"üìä –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")
    else:
        print("0 MB")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    success_rate = (len(embeddings) / len(images) * 100) if len(images) > 0 else 0
    print(f"\n‚ú® –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {success_rate:.1f}%")
    
    print("\n" + "=" * 70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())

